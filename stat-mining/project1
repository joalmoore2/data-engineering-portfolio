import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Load dataset from CSV
df = pd.read_csv(r"C:\Users\joann\Documents\WGU\D600\D600 Task 1 Dataset 1 Housing Information.csv")

#C2 Describe dependent and independent variables using descriptive statistics
# Identify quantitative and categorical variables
quantitative_vars = ["Price", "CrimeRate", "DistanceToCityCenter", "PropertyTaxRate", "LocalAmenities", "EmploymentRate", "SchoolRating"]
categorical_vars = ["IsLuxury"]  

# Calculate univariate statistics for quantitative variables
quant_stats = df[quantitative_vars].describe().T  # Transpose for readability
quant_stats["range"] = quant_stats["max"] - quant_stats["min"]  # Add range

# Calculate relative frequencies for categorical variables
cat_stats = {}
for var in categorical_vars:
    cat_stats[var] = df[var].value_counts(normalize=True).sort_values(ascending=False)  # Relative frequencies

# Display results
print("Univariate Statistics for Quantitative Variables:\n", quant_stats)
print("\nRelative Frequencies for Categorical Variables:\n")
for var, stats in cat_stats.items():
    print(f"{var}:\n{stats}\n")

#C3 Generate univariate and bivariate visuals
# Create histograms for quantitative variables
for var in quantitative_vars:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[var], bins=20, kde=True)  # Histogram with density curve
    plt.xlabel(var)
    plt.ylabel("Frequency")
    plt.title(f"Distribution of {var}")
    plt.show()

# Create bar charts for categorical variables
for var in categorical_vars:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=df[var], order=df[var].value_counts().index)
    plt.xlabel(var)
    plt.ylabel("Count")
    plt.title(f"Distribution of {var}")
    plt.show()

# Define variables
response_var = "Price"  # Dependent variable
quantitative_vars = ["CrimeRate", "DistanceToCityCenter", "PropertyTaxRate", "LocalAmenities", "EmploymentRate", "SchoolRating"]
categorical_vars = ["IsLuxury"] 

# Scatterplots for Quantitative vs. Quantitative (Explanatory vs. Response)
for var in quantitative_vars:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df[var], y=df[response_var])
    plt.xlabel(var)
    plt.ylabel(response_var)
    plt.title(f"{response_var} vs. {var}")
    plt.show()

# Side-by-Side Boxplots for Categorical vs. Quantitative (Explanatory vs. Response)
for var in categorical_vars:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[var], y=df[response_var])
    plt.xlabel(var)
    plt.ylabel(response_var)
    plt.title(f"Boxplot of {response_var} by {var}")
    plt.show()

#D1 Split data into two datasets
# Select only the relevant explanatory variables
X = df[['CrimeRate', 'DistanceToCityCenter', 'PropertyTaxRate', 
        'LocalAmenities', 'EmploymentRate', 'SchoolRating', 'IsLuxury']]

# Select the dependent variable
y = df['Price']

# Split the data into training and testing sets (80% training, 20% testing)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Save training and test datasets to CSV files
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)

train_df.to_csv("t1_training_data.csv", index=False)
test_df.to_csv("t1_test_data.csv", index=False)

print("Training and test datasets successfully saved!")

#D2 Create and optimize regression model using forward stepwise selection
import statsmodels.api as sm

# Add a constant term for the intercept
X_train_sm = sm.add_constant(X_train)  # Adds an intercept (b0) to the model

# Fit the regression model using Ordinary Least Squares (OLS)
model = sm.OLS(y_train, X_train_sm).fit()

# Display regression results
print(model.summary())

# X_train and y_train are explanatory and target variables
X_train = sm.add_constant(X_train)  # Add constant term for intercept

# Initialize an empty DataFrame for the model
initial_features = pd.DataFrame()  # DataFrame to store selected features
remaining_features = list(X_train.columns)  # All features to start with
remaining_features.remove('const')  # Remove the constant term from the feature list
best_p_value = 0.05  # Set a significance threshold for p-value

# Stepwise selection
while remaining_features:
    p_values = []
    
    # Try each remaining feature and calculate p-value for the model
    for feature in remaining_features:
        # Add the feature to the current model
        # Ensure intercept is included in the model
        temp_features = sm.add_constant(pd.concat([initial_features, X_train[[feature]]], axis=1))
        
        # Fit the model and get p-values
        model = sm.OLS(y_train, temp_features).fit()
        p_values.append((feature, model.pvalues[feature]))
    
    # Sort by p-value and select the feature with the lowest p-value
    p_values = sorted(p_values, key=lambda x: x[1])
    best_feature, best_p_value_for_feature = p_values[0]
    
    # If the best p-value is less than the threshold, keep the feature
    if best_p_value_for_feature < best_p_value:
        initial_features = pd.concat([initial_features, X_train[[best_feature]]], axis=1)
        remaining_features.remove(best_feature)
    else:
        break  # If no feature improves the model, stop
    
    print(f"Selected features: {initial_features.columns.tolist()}")
    print(f"Best p-value: {best_p_value_for_feature}")

# Final model summary
# Add intercept to final feature set before fitting the model
final_features = sm.add_constant(initial_features)
final_model = sm.OLS(y_train, final_features).fit()
print(final_model.summary())

#Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Creating a DataFrame for VIF calculation
X_optimized = X_train[['SchoolRating', 'DistanceToCityCenter', 'PropertyTaxRate', 'EmploymentRate', 'CrimeRate', 'LocalAmenities']]

# Adding intercept since VIF requires it
X_optimized = sm.add_constant(X_optimized)

# Calculating VIF for each feature
vif_data = pd.DataFrame()
vif_data["Variable"] = X_optimized.columns
vif_data["VIF"] = [variance_inflation_factor(X_optimized.values, i) for i in range(X_optimized.shape[1])]

print(vif_data)

#D3 Mean Squared Error
from sklearn.metrics import mean_squared_error

# Generate predictions on the training set
y_train_pred = final_model.predict(final_features)

# Calculate Mean Squared Error (MSE) on the training set
mse = mean_squared_error(y_train, y_train_pred)
print(f'Mean Squared Error (MSE) on the training set: {mse}')

#D4 Run prediction on test dataset
# Ensure that the test set contains only the selected features (excluding IsLuxury)
# Add intercept to the test set manually
X_test_with_intercept = sm.add_constant(X_test[initial_features.columns], has_constant='add')

# Make predictions on the test set
y_test_pred = final_model.predict(X_test_with_intercept)

# Calculate Mean Squared Error (MSE) for the test dataset
mse_test = mean_squared_error(y_test, y_test_pred)

# Output the MSE value for the test set
print(f'Mean Squared Error (MSE) on the test set: {mse_test}')

#Verification of Assumptions
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Fit a linear regression model
lin_model = LinearRegression()
lin_model.fit(X, y)
y_pred_lin = lin_model.predict(X)
mse_lin = mean_squared_error(y, y_pred_lin)

# Fit a polynomial regression model (e.g., degree 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
poly_model = LinearRegression()
poly_model.fit(X_poly, y)
y_pred_poly = poly_model.predict(X_poly)
mse_poly = mean_squared_error(y, y_pred_poly)

# Compare MSE
print(f'MSE for Linear Model: {mse_lin}')
print(f'MSE for Polynomial Model: {mse_poly}')
